services:
  # Jupyter
  jupyter-notebook:
    build:
      context: .
      dockerfile: dockerfile.jupyter
    container_name: jupyter
    environment:
      - JUPYTER_TOKEN=${JUPYTER_TOKEN}
    ports:
      - "8888:8888"
    volumes:
      - ./test:/home/jovyan/work
      - ./jars:/home/jovyan/work/jars
    networks:
      - project-network
  #Kafka & Kafka UI:
  kafka:
    image: bitnami/kafka:4.0.0
    container_name: kafka
    restart: unless-stopped
    env_file:
      - ./envs/kafka.env
    ports:
      - "9092:9092"
    networks:
      - project-network
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
    environment:
      - KAFKA_CLUSTERS_0_NAME=kafka
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092
    ports:
      - "8081:8080"
    networks:
      - project-network
  # Spark:
  spark-master:
    image: bitnami/spark:3.5.0
    container_name: spark-master
    hostname: spark-master
    restart: unless-stopped
    env_file:
      - ./envs/spark-master.env
    ports:
      - "7077:7077"
      - "8080:8080"
    networks:
      - project-network
  spark-worker:
    image: bitnami/spark:3.5.0
    container_name: spark-worker
    hostname: spark-worker
    restart: unless-stopped
    depends_on:
      - spark-master
    env_file:
      - ./envs/spark-worker.env
    environment:
      - SPARK_LOCAL_IP=spark-worker
    networks:
      - project-network
  # Hadoop
  namenode:
    image: apache/hadoop:3.3.6
    container_name: namenode
    hostname: namenode
    command: [ "hdfs", "namenode" ]
    env_file:
      - ./envs/hadoop.env
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
    ports:
      - "9870:9870"
    networks:
      - project-network
  datanode:
    image: apache/hadoop:3.3.6
    container_name: datanode
    command: [ "hdfs", "datanode" ]
    depends_on:
      - namenode
    env_file:
      - ./envs/hadoop.env
    networks:
      - project-network
  # Streaming Process
  streaming_data:
    build:
      context: .
      dockerfile: dockerfile.python
    container_name: streaming_data
    restart: unless-stopped
    depends_on:
      - kafka
    volumes:
      - ./streaming:/streaming
    command: [ "sh", "-c", "python streaming_data.py"]
    networks:
      - project-network
  streaming_transform:
    build:
      context: .
      dockerfile: dockerfile.python
    container_name: streaming_transform
    restart: unless-stopped
    depends_on:
      - streaming_data
    volumes:
      - ./streaming:/streaming
      - ./jars:/streaming/jars
    command: [ "sh", "-c", "python streaming_transform.py"]
    networks:
      - project-network
  # Postgres
  postgres:
    image: postgres:16.4
    container_name: postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_MULTIPLE_DATABASES: ${POSTGRES_DATABASES}
    ports:
      - "5432:5432"
    volumes:
      - E:/postgres/data:/var/lib/postgresql/data
      - ./initdb:/docker-entrypoint-initdb.d
    networks:
      - project-network
  adminer:
    image: adminer
    container_name: postgres-express
    depends_on:
      - postgres
    restart: always
    ports:
      - "8082:8080"
    networks:
      - project-network
  # Airflow
  airflow-webserver:
    build:
      context: .
      dockerfile: dockerfile.airflow
    container_name: airflow-webserver
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY}
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      PYTHONPATH: /opt/airflow
      TZ: Asia/Ho_Chi_Minh
    volumes:
      - ./dags:/opt/airflow/dags
      - ./jars:/opt/airflow/jars
    ports:
      - 8083:8080
    command: >
      bash -c "
        airflow db init &&
        airflow users create --username ${AIRFLOW_USER} --password ${AIRFLOW_PASSWORD} --firstname ${AIRFLOW_FIRSTNAME} --lastname ${AIRFLOW_LASTNAME} --role Admin --email ${AIRFLOW_EMAIL} &&
        airflow webserver
      "
    restart: unless-stopped
    networks:
      - project-network
  airflow-scheduler:
    build:
      context: .
      dockerfile: dockerfile.airflow
    container_name: airflow-scheduler
    depends_on:
      - airflow-webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY}
      TZ: Asia/Ho_Chi_Minh
      PYTHONPATH: /opt/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./jars:/opt/airflow/jars
    command: airflow scheduler
    restart: unless-stopped
    networks:
      - project-network

networks:
  project-network:
